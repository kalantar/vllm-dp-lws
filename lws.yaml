apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
    name: vllm
spec:
    replicas: 1
    leaderWorkerTemplate:
        size: 2
        restartPolicy: RecreateGroupOnPodRestart
        leaderTemplate:
            metadata:
                labels:
                    role: leader
            spec:
              initContainers:
              - name: vllm-source-installer
                image: "quay.io/tms/vllm-dev-base:0.0.12" # Your base image
                imagePullPolicy: Always
                command: ["/init-scripts/init-vllm.sh"]
                env:
                  - name: GH_TOKEN_FROM_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: gh-token-secret
                        key: GH_TOKEN
                        optional: true
                volumeMounts:
                  - name: code-storage
                    mountPath: /app/code
                  - name: init-scripts-volume # Mounts the directory containing input scripts
                    mountPath: /init-scripts
                resources:
                  requests:
                    cpu: "4"
                    memory: "16Gi"
                  limits:
                    cpu: "4"
                    memory: "16Gi"

              containers:
              - name: vllm-leader
                image: "quay.io/tms/vllm-dev-base:0.0.12" # Use the same base image
                imagePullPolicy: Always
                workingDir: /app/code
                stdin: true
                tty: true
                command: ["/bin/sh","-c"]
                args:
                  - |
                    GLOO_SOCKET_IFNAME=$(LWS_LEADER_ADDRESS)
                    NCCL_SOCKET_IFNAME=$(LWS_LEADER_ADDRESS)
                    NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME=$(LWS_LEADER_ADDRESS)
                    DP_SIZE=4
                    TP_SIZE=1
                    DP_SIZE_LOCAL=2
                    exec /app/code/venv/bin/vllm serve \
                      deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
                      --port 8080 \
                      --disable-log-requests \
                      --enforce-eager \
                      --enable-expert-parallel \
                      --tensor-parallel-size $TP_SIZE \
                      --data-parallel-size $DP_SIZE \
                      --data-parallel-size-local $DP_SIZE_LOCAL \
                      --data-parallel-address $(LWS_LEADER_ADDRESS) \
                      --data-parallel-rpc-port 5555 \
                      --trust-remote-code
                env:
#                  - name: VLLM_ALL2ALL_BACKEND
#                    value: "pplx"
                  - name: VLLM_LOGGING_LEVEL
                    value: "DEBUG"
                  - name: HF_TOKEN
                    valueFrom:
                      secretKeyRef:
                        name: hf-secret
                        key: HF_TOKEN
                        optional: true
                  - name: GH_TOKEN_FROM_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: gh-token-secret
                        key: GH_TOKEN
                        optional: true

                resources:
                  limits:
                    nvidia.com/gpu: "4"
                    memory: 64Gi
                    ephemeral-storage: 128Gi
                    rdma/ib: 1
                  requests:
                    cpu: 8
                    memory: 64Gi
                    ephemeral-storage: 128Gi
                    nvidia.com/gpu: "4"
                    rdma/ib: 1
                ports:
                  - containerPort: 8080
                readinessProbe:
                  tcpSocket:
                    port: 8080
                  initialDelaySeconds: 600
                  periodSeconds: 600
                volumeMounts:
                  - name: code-storage
                    mountPath: /app/code
                  - mountPath: /dev/shm
                    name: dshm
              volumes:
                - name: code-storage
                  emptyDir: {}
                # Volume for the init script from ConfigMap
                - name: init-scripts-volume
                  configMap:
                    name: vllm-init-scripts-config
                    defaultMode: 0755 # Set execute permissions for the script
                - name: dshm
                  emptyDir:
                    medium: Memory
                    sizeLimit: 1Gi


        workerTemplate:
            spec:
              initContainers:
              - name: vllm-source-installer
                image: "quay.io/tms/vllm-dev-base:0.0.12" # Your base image
                imagePullPolicy: Always
                command: ["/init-scripts/init-vllm.sh"]
                env:
                  - name: GH_TOKEN_FROM_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: gh-token-secret
                        key: GH_TOKEN
                        optional: true
                volumeMounts:
                  - name: code-storage
                    mountPath: /app/code
                  - name: init-scripts-volume # Mounts the directory containing input scripts
                    mountPath: /init-scripts
                resources:
                  requests:
                    cpu: "4"
                    memory: "16Gi"
                  limits:
                    cpu: "4"
                    memory: "16Gi"

              containers:
              - name: vllm-worker
                image: "quay.io/tms/vllm-dev-base:0.0.12" # Use the same base image
                imagePullPolicy: Always
                workingDir: /app/code
                stdin: true
                tty: true
                command: ["/bin/sh","-c"]
                args:
                  - |
                    GLOO_SOCKET_IFNAME=$(LWS_LEADER_ADDRESS)
                    NCCL_SOCKET_IFNAME=$(LWS_LEADER_ADDRESS)
                    NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME=$(LWS_LEADER_ADDRESS)
                    DP_SIZE=4
                    TP_SIZE=1
                    DP_SIZE_LOCAL=2
                    START_RANK=$(( LWS_WORKER_INDEX * DP_SIZE_LOCAL ))
                    exec /app/code/venv/bin/vllm serve \
                      deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
                      --port 8080 \
                      --disable-log-requests \
                      --enforce-eager \
                      --enable-expert-parallel \
                      --tensor-parallel-size $TP_SIZE \
                      --data-parallel-size $DP_SIZE \
                      --data-parallel-size-local $DP_SIZE_LOCAL \
                      --data-parallel-address $(LWS_LEADER_ADDRESS) \
                      --data-parallel-rpc-port 5555 \
                      --headless \
                      --data-parallel-start-rank $START_RANK \
                      --trust-remote-code
                env:
#                  - name: VLLM_ALL2ALL_BACKEND
#                    value: "pplx"
                  - name: VLLM_LOGGING_LEVEL
                    value: "DEBUG"
                  - name: HF_TOKEN
                    valueFrom:
                      secretKeyRef:
                        name: hf-secret
                        key: HF_TOKEN
                        optional: true
                  - name: GH_TOKEN_FROM_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: gh-token-secret
                        key: GH_TOKEN
                        optional: true

                resources:
                  limits:
                    nvidia.com/gpu: "4"
                    memory: 64Gi
                    ephemeral-storage: 128Gi
                    rdma/ib: 1
                  requests:
                    cpu: 8
                    memory: 64Gi
                    ephemeral-storage: 128Gi
                    nvidia.com/gpu: "4"
                    rdma/ib: 1
                volumeMounts:
                  - name: code-storage
                    mountPath: /app/code
                  - mountPath: /dev/shm
                    name: dshm
              volumes:
                # Shared volume for vLLM source code, cloned by init container into /app/vllm
                - name: code-storage # This emptyDir will be mounted at /app/vllm
                  emptyDir: {}
                # Volume for the init script from ConfigMap
                - name: init-scripts-volume
                  configMap:
                    name: vllm-init-scripts-config
                    defaultMode: 0755 # Set execute permissions for the script
                - name: dshm
                  emptyDir:
                    medium: Memory
                    sizeLimit: 1Gi
---
apiVersion: v1
kind: Service
metadata:
    name: vllm-leader
spec:
    ports:
        - name: http
          port: 8080
          protocol: TCP
          targetPort: 8080
    selector:
        leaderworkerset.sigs.k8s.io/name: vllm
        role: leader
    type: ClusterIP
