apiVersion: leaderworkerset.x-k8s.io/v1
kind: LeaderWorkerSet
metadata:
    name: __LWS_NAME__
spec:
    replicas: 1
    leaderWorkerTemplate:
        size: 2
        restartPolicy: RecreateGroupOnPodRestart
        leaderTemplate:
            metadata:
                labels:
                    role: leader
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                      - matchExpressions:
                          - key: gpu.nvidia.com/model
                            operator: In
                            values:
                              - H200

              initContainers:
              - name: vllm-source-installer
                image: "quay.io/tms/vllm-dev-base:0.0.14" # Your base image
                imagePullPolicy: Always
                command: ["/init-scripts/init-vllm.sh"]
                env:
                  - name: VLLM_REPO_URL
                    value: "https://github.com/neuralmagic/vllm.git"
                  - name: VLLM_BRANCH
                    value: "varun/deepep"
                  - name: GH_TOKEN_FROM_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: gh-token-secret
                        key: GH_TOKEN
                        optional: true
                volumeMounts:
                  - name: code-storage
                    mountPath: /app/code
                  - name: init-scripts-volume # Mounts the directory containing input scripts
                    mountPath: /init-scripts
                resources:
                  requests:
                    cpu: "4"
                    memory: "16Gi"
                  limits:
                    cpu: "4"
                    memory: "16Gi"

              containers:
              - name: vllm-leader
                image: "quay.io/tms/vllm-dev-base:0.0.14" # Use the same base image
                imagePullPolicy: Always
                workingDir: /app/code
                stdin: true
                tty: true
                command: ["/bin/sh","-c"]
                args:
                  - |
                    DP_SIZE=2
                    TP_SIZE=1
                    DP_SIZE_LOCAL=1
                    exec /app/code/venv/bin/vllm serve \
                      deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
                      --port 8080 \
                      --disable-log-requests \
                      --enforce-eager \
                      --kv-transfer-config \
                        '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                      --enable-expert-parallel \
                      --tensor-parallel-size $TP_SIZE \
                      --data-parallel-size $DP_SIZE \
                      --data-parallel-size-local $DP_SIZE_LOCAL \
                      --data-parallel-address $(LWS_LEADER_ADDRESS) \
                      --data-parallel-rpc-port 5555 \
                      --trust-remote-code
                env:
                  - name: VLLM_ALL2ALL_BACKEND
#                    value: "naive"
#                    value: "pplx"
#                    value: "deepep_high_throughput"
                    value: "deepep_low_latency"
#
                    # Needed for GDRCOPY to be used.
                    # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                  - name: NVIDIA_GDRCOPY
                    value: "enabled"
#                  - name: NVIDIA_NVSWITCH
#                    value: "enabled"
#                  - name: NVIDIA_GDS # for GPU direct storage.
#                    value: "enabled"
#                  - name: NVIDIA_MOFED # Crashes with this.
#                    value: "enabled"
                  - name: NCCL_DEBUG
                    value: "INFO"
                  - name: NVSHMEM_DEBUG
                    value: "INFO"
                  - name: NVSHMEM_REMOTE_TRANSPORT
                    value: "ibrc"
#                  - name: NVSHMEM_IB_ENABLE_IBGDA
#                    value: "true"
#                  - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
#                    value: "true"
#                  - name: NVSHMEM_HCA_LIST
#                    value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                  - name: GLOO_SOCKET_IFNAME
                    value: "eth0"
                  - name: NCCL_SOCKET_IFNAME
                    value: "eth0"
                  - name: NCCL_IB_HCA
                    value: "ibp"
                  - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                    value: "eth0"
                  - name: VLLM_LOGGING_LEVEL
                    value: "DEBUG"
                  - name: HF_TOKEN
                    valueFrom:
                      secretKeyRef:
                        name: hf-secret
                        key: HF_TOKEN
                        optional: true
                  - name: GH_TOKEN_FROM_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: gh-token-secret
                        key: GH_TOKEN
                        optional: true

                securityContext:
                  capabilities:
                    add: [ "IPC_LOCK" ]
                resources:
                  limits:
                    nvidia.com/gpu: "1"
                    memory: 64Gi
                    ephemeral-storage: 128Gi
                    rdma/ib: 1
                  requests:
                    cpu: 8
                    memory: 64Gi
                    ephemeral-storage: 128Gi
                    nvidia.com/gpu: "1"
                    rdma/ib: 1
                ports:
                  - containerPort: 8080
                readinessProbe:
                  tcpSocket:
                    port: 8080
                  initialDelaySeconds: 30
                  periodSeconds: 30
                volumeMounts:
                  - name: code-storage
                    mountPath: /app/code
                  - mountPath: /dev/shm
                    name: dshm
              volumes:
                - name: code-storage
                  emptyDir: {}
                # Volume for the init script from ConfigMap
                - name: init-scripts-volume
                  configMap:
                    name: vllm-init-scripts-config
                    defaultMode: 0755 # Set execute permissions for the script
                # Needed for NCCL to function
                - name: dshm
                  emptyDir:
                    medium: Memory
                    sizeLimit: 1Gi


        workerTemplate:
            spec:
              affinity:
                nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                    nodeSelectorTerms:
                      - matchExpressions:
                          - key: gpu.nvidia.com/model
                            operator: In
                            values:
                              - H200

              initContainers:
              - name: vllm-source-installer
                image: "quay.io/tms/vllm-dev-base:0.0.14" # Your base image
                imagePullPolicy: Always
                command: ["/init-scripts/init-vllm.sh"]
                env:
                  - name: VLLM_REPO_URL
                    value: "https://github.com/neuralmagic/vllm.git"
                  - name: VLLM_BRANCH
                    value: "varun/deepep"
                  - name: GH_TOKEN_FROM_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: gh-token-secret
                        key: GH_TOKEN
                        optional: true
                volumeMounts:
                  - name: code-storage
                    mountPath: /app/code
                  - name: init-scripts-volume # Mounts the directory containing input scripts
                    mountPath: /init-scripts
                resources:
                  requests:
                    cpu: "4"
                    memory: "16Gi"
                  limits:
                    cpu: "4"
                    memory: "16Gi"

              containers:
              - name: vllm-worker
                image: "quay.io/tms/vllm-dev-base:0.0.14" # Use the same base image
                imagePullPolicy: Always
                workingDir: /app/code
                stdin: true
                tty: true
                command: ["/bin/sh","-c"]
                args:
                  - |
                    DP_SIZE=2
                    TP_SIZE=1
                    DP_SIZE_LOCAL=1
                    START_RANK=$(( LWS_WORKER_INDEX * DP_SIZE_LOCAL ))
                    exec /app/code/venv/bin/vllm serve \
                      deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
                      --port 8080 \
                      --disable-log-requests \
                      --enforce-eager \
                      --kv-transfer-config \
                        '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                      --enable-expert-parallel \
                      --tensor-parallel-size $TP_SIZE \
                      --data-parallel-size $DP_SIZE \
                      --data-parallel-size-local $DP_SIZE_LOCAL \
                      --data-parallel-address $(LWS_LEADER_ADDRESS) \
                      --data-parallel-rpc-port 5555 \
                      --headless \
                      --data-parallel-start-rank $START_RANK \
                      --trust-remote-code
                env:
                  - name: VLLM_ALL2ALL_BACKEND
#                    value: "naive"
#                    value: "pplx"
#                    value: "deepep_high_throughput"
                    value: "deepep_low_latency"
#
                    # Needed for GDRCOPY to be used.
                    # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                  - name: NVIDIA_GDRCOPY
                    value: "enabled"
#                  - name: NVIDIA_NVSWITCH
#                    value: "enabled"
#                  - name: NVIDIA_GDS
#                    value: "enabled"
#                  - name: NVIDIA_MOFED
#                    value: "enabled"
                  - name: NCCL_DEBUG
                    value: "INFO"
                  - name: NVSHMEM_DEBUG
                    value: "INFO"
                  - name: NVSHMEM_REMOTE_TRANSPORT
                    value: "ibrc"
#                  - name: NVSHMEM_IB_ENABLE_IBGDA
#                    value: "true"
#                  - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
#                    value: "true"
#                  - name: NVSHMEM_HCA_LIST
#                    value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                  - name: GLOO_SOCKET_IFNAME
                    value: "eth0"
                  - name: NCCL_SOCKET_IFNAME
                    value: "eth0"
                  - name: NCCL_IB_HCA
                    value: "ibp"
                  - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                    value: "eth0"
                  - name: VLLM_LOGGING_LEVEL
                    value: "DEBUG"
                  - name: HF_TOKEN
                    valueFrom:
                      secretKeyRef:
                        name: hf-secret
                        key: HF_TOKEN
                        optional: true
                  - name: GH_TOKEN_FROM_SECRET
                    valueFrom:
                      secretKeyRef:
                        name: gh-token-secret
                        key: GH_TOKEN
                        optional: true

                securityContext:
                  capabilities:
                    add: [ "IPC_LOCK" ]
                resources:
                  limits:
                    nvidia.com/gpu: "1"
                    memory: 64Gi
                    ephemeral-storage: 128Gi
                    rdma/ib: 1
                  requests:
                    cpu: 8
                    memory: 64Gi
                    ephemeral-storage: 128Gi
                    nvidia.com/gpu: "1"
                    rdma/ib: 1
                volumeMounts:
                  - name: code-storage
                    mountPath: /app/code
                  - mountPath: /dev/shm
                    name: dshm
              volumes:
                # Shared volume for vLLM source code, cloned by init container into /app/vllm
                - name: code-storage # This emptyDir will be mounted at /app/vllm
                  emptyDir: {}
                # Volume for the init script from ConfigMap
                - name: init-scripts-volume
                  configMap:
                    name: vllm-init-scripts-config
                    defaultMode: 0755 # Set execute permissions for the script
                # Needed for NCCL to function
                - name: dshm
                  emptyDir:
                    medium: Memory
                    sizeLimit: 1Gi

---
apiVersion: v1
kind: Service
metadata:
    name: __SERVICE_NAME__
spec:
    ports:
        - name: http
          port: 8080
          protocol: TCP
          targetPort: 8080
    selector:
        leaderworkerset.sigs.k8s.io/name: __LWS_NAME__
        role: leader
    type: ClusterIP
